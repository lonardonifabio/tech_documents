<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document excerpt discusses the application of neural networks and related models, specifically focusing on approaches to mitigate the 'curse of dimensionality' when dealing with large datasets. It highlights the limitations of linear models based on fixed basis functions, particularly their ..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Neural Networks and Basis Functions</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Neural Networks and Basis Functions"><meta property="og:description" content="This document excerpt discusses the application of neural networks and related models, specifically focusing on approaches to mitigate the 'curse of dimensionality' when dealing with large datasets. It highlights the limitations of linear models based on fixed basis functions, particularly their ..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/9fc8b08ba09814d578c6be62a6295b36"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Neural Networks and Basis Functions - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="Neural Networks, Support Vector Machines (SVMs), Basis Functions, Dimensionality Curse, Probabilistic Outputs, Parametric, neural networks, skip-layer connections, adaptive weights, bias parameter"><meta property="article:published_time" content="2025-06-12T00:07:44.011527"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content=""><meta name="keywords" content="Neural Networks, Support Vector Machines (SVMs), Basis Functions, Dimensionality Curse, Probabilistic Outputs, Parametric, neural networks, skip-layer connections, adaptive weights, bias parameter"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/9fc8b08ba09814d578c6be62a6295b36"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=9fc8b08ba09814d578c6be62a6295b36";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Neural Networks and Basis Functions</h1>  <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This document excerpt discusses the application of neural networks and related models, specifically focusing on approaches to mitigate the &#39;curse of dimensionality&#39; when dealing with large datasets. It highlights the limitations of linear models based on fixed basis functions, particularly their struggle with high-dimensional data. The excerpt introduces Support Vector Machines (SVMs) and Relevance Vector Machines (RVMs) as solutions. SVMs are described as employing basis functions centered around training data points and selecting a subset during training, leveraging a convex optimization process despite nonlinear training. The key advantage of SVMs is the resulting sparsity of the model compared to the number of training points. RVMs are presented as an alternative that also selects a subset of basis functions but produces probabilistic outputs, albeit through a nonconvex optimization.  The document emphasizes that the number of basis functions typically increases with the size of the training set.  The core concept explored is adapting basis functions to data to improve model applicability and reduce complexity, a common strategy in machine learning to manage the challenges posed by high-dimensional data. The excerpt sets the stage for a deeper dive into these techniques, likely within the context of neural network architectures and their ability to learn complex patterns from data.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">Adaptive Basis Functions: Models that adjust basis functions based on the data.</li><li style="margin-bottom: 5px;">Curse of Dimensionality: A challenge in machine learning where model performance degrades as the number of features increases.</li><li style="margin-bottom: 5px;">Convex Optimization: The objective function in SVM training is convex, leading to a straightforward solution.</li><li style="margin-bottom: 5px;">Sparse Models: Models with a reduced number of basis functions, often achieved through techniques like Relevance Vector Machines.</li><li style="margin-bottom: 5px;">Two-layer network: Defined by the number of layers of adaptive weights.</li><li style="margin-bottom: 5px;">Hidden units: Units within a layer that process information.</li><li style="margin-bottom: 5px;">Feed-forward topology: A network architecture where information flows in one direction.</li><li style="margin-bottom: 5px;">Sigmoidal hidden units: Hidden units with a sigmoid activation function.</li><li style="margin-bottom: 5px;">Conditional probability p(C1|x) and p(C2|x)</li><li style="margin-bottom: 5px;">Bernoulli distribution for target values given inputs</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Neural Networks </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Support Vector Machines (SVMs) </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Basis Functions </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Dimensionality Curse </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Probabilistic Outputs </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Parametric </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> neural networks </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> skip-layer connections </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> adaptive weights </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> bias parameter </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> AI </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Part_2_Pattern_Recognition_and_Machine_Learning_1630696123.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
üì• Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=9fc8b08ba09814d578c6be62a6295b36" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
üîç View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=9fc8b08ba09814d578c6be62a6295b36" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>