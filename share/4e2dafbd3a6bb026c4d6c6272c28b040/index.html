<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document excerpt discusses the fundamentals of automatic differentiation, specifically focusing on reverse mode and its application within machine learning. It primarily addresses unconstrained convex optimization problems, a cornerstone of many machine learning algorithms. The core concept ..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Reverse Mode of Automatic Differentiation</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Reverse Mode of Automatic Differentiation"><meta property="og:description" content="This document excerpt discusses the fundamentals of automatic differentiation, specifically focusing on reverse mode and its application within machine learning. It primarily addresses unconstrained convex optimization problems, a cornerstone of many machine learning algorithms. The core concept ..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/4e2dafbd3a6bb026c4d6c6272c28b040"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Reverse Mode of Automatic Differentiation - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Machine Learning"><meta property="article:tag" content="optimization, gradient descent, machine learning, convex analysis, stochastic gradient method, automatic differentiation, gradient information, convex optimization, empirical risk, regression"><meta property="article:published_time" content="2025-07-03T07:05:53.018601"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content=""><meta name="keywords" content="optimization, gradient descent, machine learning, convex analysis, stochastic gradient method, automatic differentiation, gradient information, convex optimization, empirical risk, regression"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/4e2dafbd3a6bb026c4d6c6272c28b040"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=4e2dafbd3a6bb026c4d6c6272c28b040";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.DNY9BzS4.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Reverse Mode of Automatic Differentiation</h1>  <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This document excerpt discusses the fundamentals of automatic differentiation, specifically focusing on reverse mode and its application within machine learning. It primarily addresses unconstrained convex optimization problems, a cornerstone of many machine learning algorithms. The core concept revolves around devising &#39;cheap&#39; algorithms that utilize gradient information to approximate a minimizer when one exists. The excerpt defines key terms like &#39;argmin&#39; and clarifies the notation used to represent optimization problems, distinguishing between cases where a minimizer exists and when it doesn&#39;t. It introduces the empirical risk, a common component in learning scenarios, particularly for regression and classification problems. The document highlights the use of first-order algorithms, which rely on gradient information, and emphasizes the importance of minimizing computational cost per iteration. The inclusion of Figure 1, depicting linear regression, a linear classifier, and a loss function for classification, further illustrates the practical application of these concepts. The discussion of &#39;parameter&#39; in the context of learning scenarios suggests an exploration of model complexity and its impact on optimization.  The excerpt lays the groundwork for understanding how automatic differentiation enables efficient training of machine learning models by allowing for the calculation of gradients, which are then used to update model parameters iteratively.  The focus on unconstrained convex optimization provides a theoretical framework for understanding the underlying principles of optimization in machine learning.  It sets the stage for delving deeper into specific algorithms and techniques for gradient-based optimization.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">First order optimization methods</li><li style="margin-bottom: 5px;">Unconstrained optimization in machine learning</li><li style="margin-bottom: 5px;">Convexity and solutions of optimization problems</li><li style="margin-bottom: 5px;">In most part of this Chapter, we consider unconstrained convex optimization problems of the form inf x2Rpf(x); (1) and try to devise &quot;cheap&quot; algorithms with a low computational cost per iteration to approximate a minimizer when it exists.</li><li style="margin-bottom: 5px;">In typical learning scenario, f(x) is the empirical risk for regression or classi cation, and pis the number of parameter.</li><li style="margin-bottom: 5px;">argmin xf(x)def.=fx2Rp;f(x) = inffg; 2</li><li style="margin-bottom: 5px;">The Moore-Penrose pseudo-inverse is a generalization of the inverse for non-square matrices.</li><li style="margin-bottom: 5px;">The kernel of a matrix (ker(A)) represents the nullspace of the matrix, corresponding to the solutions of Ax = 0.</li><li style="margin-bottom: 5px;">The condition ker(A) = {0} implies that the matrix A is invertible.</li><li style="margin-bottom: 5px;">PCA (Principal Component Analysis) is a dimensionality reduction technique.</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> optimization </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> gradient descent </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> machine learning </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> convex analysis </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> stochastic gradient method </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> automatic differentiation </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> gradient information </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> convex optimization </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> empirical risk </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> regression </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> Machine Learning </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Optimization_for_Machine_Learning_1630616567.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
📥 Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=4e2dafbd3a6bb026c4d6c6272c28b040" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
🔍 View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=4e2dafbd3a6bb026c4d6c6272c28b040" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>