<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncon..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Two types of AI existential risk: decisive and accumulative</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Two types of AI existential risk: decisive and accumulative"><meta property="og:description" content="This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncon..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/8ae5adf277039d0865e209e3af78ece7"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Two types of AI existential risk: decisive and accumulative - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="AI existential risk, x-risks, superintelligence, complex systems analysis, incremental disruptions, systemic resilience, AGI, ASI, instrumental convergence, paperclip production"><meta property="article:published_time" content="2025-06-27T22:32:59.411631"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content="Atoosa Kasirzadeh"><meta name="keywords" content="AI existential risk, x-risks, superintelligence, complex systems analysis, incremental disruptions, systemic resilience, AGI, ASI, instrumental convergence, paperclip production"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/8ae5adf277039d0865e209e3af78ece7"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.bQYxGZ17.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Two types of AI existential risk: decisive and accumulative</h1> <div style="margin-bottom: 15px;"> <p style="color: #6b7280;"> <strong>Authors:</strong> Atoosa Kasirzadeh </p> </div> <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncontrolled superintelligence and the potential for human extinction or irreversible societal collapse. However, the paper argues for a contrasting perspective – an ‘accumulative AI x-risk’ hypothesis. This hypothesis posits that existential threats from AI don&#39;t necessarily manifest through a single, dramatic takeover but rather through a gradual, incremental accumulation of vulnerabilities and disruptions. The author frames this as a ‘boiling frog’ scenario, where small, seemingly insignificant AI-induced risks accumulate over time, steadily eroding systemic and societal resilience. The paper highlights the danger of overlooking the potential for AI to destabilize economic and political structures through a series of interconnected, low-level threats. It emphasizes the importance of considering the long-term, gradual impacts of AI development, rather than solely focusing on immediate, dramatic scenarios. The research suggests that a failure to recognize this accumulative risk could lead to a situation where a triggering event, born from the accumulated vulnerabilities, results in an irreversible collapse of societal systems. The analysis draws upon complex systems theory to illustrate the dynamics of this potential threat, urging a more nuanced and comprehensive approach to assessing and mitigating AI x-risks.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">Decisive AI x-risk hypothesis: Focuses on abrupt, catastrophic events caused by advanced AI.</li><li style="margin-bottom: 5px;">Accumulative AI x-risk hypothesis:  Focuses on gradual, incremental threats leading to irreversible collapse.</li><li style="margin-bottom: 5px;">Boiling Frog Scenario:  A metaphor representing the slow, cumulative nature of AI risks undermining societal resilience.</li><li style="margin-bottom: 5px;">The concept of AGI representing human-level intelligence.</li><li style="margin-bottom: 5px;">The concept of ASI denoting intelligence surpassing human intelligence.</li><li style="margin-bottom: 5px;">The idea of instrumental convergence – AI pursuing goals that unintentionally lead to negative outcomes.</li><li style="margin-bottom: 5px;">The thought experiment illustrating x-risks with a simple, yet potentially dangerous, goal.</li><li style="margin-bottom: 5px;">The paper&#39;s focus on conventional AI x-risk discussions does not imply specific predictions about timing or severity.</li><li style="margin-bottom: 5px;">Catastrophic events can be either decisive (like nuclear weapons) or accumulative (like climate change).</li><li style="margin-bottom: 5px;">The time axis in the figure is illustrative and captures qualitative differences in catastrophic events.</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> AI existential risk </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> x-risks </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> superintelligence </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> complex systems analysis </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> incremental disruptions </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> systemic resilience </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> AGI </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> ASI </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> instrumental convergence </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> paperclip production </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> AI </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Two_types_of_AI_existential_risk_1748125530.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
📥 Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
🔍 View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>