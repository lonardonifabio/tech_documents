<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document, published by the National Institute of Standards and Technology (NIST), provides a comprehensive taxonomy and terminology related to adversarial machine learning attacks and their corresponding mitigations.  It‚Äôs a crucial resource for researchers, practitioners, and policymakers i..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations"><meta property="og:description" content="This document, published by the National Institute of Standards and Technology (NIST), provides a comprehensive taxonomy and terminology related to adversarial machine learning attacks and their corresponding mitigations.  It‚Äôs a crucial resource for researchers, practitioners, and policymakers i..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/4cea72e04c8d97c244e25b2cf14a1c2c"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="AI, Adversarial Machine Learning, Taxonomy, Terminology, Attacks, Mitigations, NIST, Trustworthy AI, Responsible AI, attacker capabilities"><meta property="article:published_time" content="2025-07-03T13:27:04.709111"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content="Alina Oprea, Apostol Vassilev, Maia Hamin, Alie Fordyce, Hyrum Anderson, Xander Davies"><meta name="keywords" content="AI, Adversarial Machine Learning, Taxonomy, Terminology, Attacks, Mitigations, NIST, Trustworthy AI, Responsible AI, attacker capabilities"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/4cea72e04c8d97c244e25b2cf14a1c2c"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=4cea72e04c8d97c244e25b2cf14a1c2c";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.bQYxGZ17.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations</h1> <div style="margin-bottom: 15px;"> <p style="color: #6b7280;"> <strong>Authors:</strong> Alina Oprea, Apostol Vassilev, Maia Hamin, Alie Fordyce, Hyrum Anderson, Xander Davies </p> </div> <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This document, published by the National Institute of Standards and Technology (NIST), provides a comprehensive taxonomy and terminology related to adversarial machine learning attacks and their corresponding mitigations.  It‚Äôs a crucial resource for researchers, practitioners, and policymakers involved in ensuring the trustworthiness and responsible development of AI systems. The publication focuses on systematically categorizing various attack techniques targeting machine learning models, ranging from simple input perturbations to more sophisticated methods designed to exploit vulnerabilities.  It details the different types of attacks, including those targeting specific model architectures and training procedures.  Furthermore, the document outlines a range of mitigation strategies, encompassing techniques for robust training, adversarial detection, and model verification.  The taxonomy is designed to facilitate a deeper understanding of the threat landscape and enable the development of more resilient AI systems.  The NIST AI 100-2e2025 series is a key initiative aimed at establishing standards and best practices for trustworthy AI. This specific document contributes significantly to that effort by providing a detailed and organized framework for addressing the challenges posed by adversarial machine learning. The inclusion of organizations like the U.S. AI Safety Institute, the U.K. AI Security Institute, and the National Institute of Standards and Robust Intelligence (now Cisco) highlights the collaborative nature of this research and the global importance of addressing these vulnerabilities. The document&#39;s availability free of charge underscores NIST&#39;s commitment to promoting open access to critical information in the field of AI security.  The inclusion of commercial equipment and instruments is noted for the purpose of accurately describing the experimental setup, with a clear disclaimer that this does not constitute an endorsement of any specific product or service.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">A taxonomy and terminology for attacks and mitigations in adversarial machine learning.</li><li style="margin-bottom: 5px;">The NIST&#39;s efforts in developing trustworthy and responsible AI standards.</li><li style="margin-bottom: 5px;">Identification of commercial equipment and materials for experimental purposes, without endorsement by NIST.</li><li style="margin-bottom: 5px;">Attacker capabilities refer to the abilities of an attacker.</li><li style="margin-bottom: 5px;">Supply chain attacks involve compromising systems through vulnerable components.</li><li style="margin-bottom: 5px;">Data poisoning attacks manipulate training data.</li><li style="margin-bottom: 5px;">Model poisoning attacks target machine learning models.</li><li style="margin-bottom: 5px;">Prompt injection attacks exploit vulnerabilities in prompt-based systems.</li><li style="margin-bottom: 5px;">Indirect prompt injection attacks involve manipulating systems through indirect means.</li><li style="margin-bottom: 5px;">Availability attacks aim to disrupt system availability.</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> AI </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Adversarial Machine Learning </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Taxonomy </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Terminology </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Attacks </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Mitigations </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> NIST </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Trustworthy AI </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Responsible AI </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> attacker capabilities </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> AI </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/AI_NIST_Report_1743720199.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
üì• Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=4cea72e04c8d97c244e25b2cf14a1c2c" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
üîç View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=4cea72e04c8d97c244e25b2cf14a1c2c" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>