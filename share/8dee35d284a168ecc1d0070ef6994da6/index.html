<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document excerpt presents introductory lecture notes for a course titled &#34;Intro to Deep Learning,&#34; developed by the Hebrew University of Jerusalem. The course material is based on lectures delivered by Raanan Fattal and recitations led by Matan Halfon. The notes cover fundamental concepts wi..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Intro to Deep Learning</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Intro to Deep Learning"><meta property="og:description" content="This document excerpt presents introductory lecture notes for a course titled &#34;Intro to Deep Learning,&#34; developed by the Hebrew University of Jerusalem. The course material is based on lectures delivered by Raanan Fattal and recitations led by Matan Halfon. The notes cover fundamental concepts wi..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/8dee35d284a168ecc1d0070ef6994da6"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Intro to Deep Learning - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="Deep Learning, Information Theory, Neural Network, Gradient Descent, Mini Batching, SGD, Likelihood, Regression Loss, Classification Loss, Cycle-GAN"><meta property="article:published_time" content="2025-07-02T21:59:25.867411"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content="Hadar Sharvit, Raanan Fattal, Matan Halfon"><meta name="keywords" content="Deep Learning, Information Theory, Neural Network, Gradient Descent, Mini Batching, SGD, Likelihood, Regression Loss, Classification Loss, Cycle-GAN"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/8dee35d284a168ecc1d0070ef6994da6"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=8dee35d284a168ecc1d0070ef6994da6";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.8gVsPnqW.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Intro to Deep Learning</h1> <div style="margin-bottom: 15px;"> <p style="color: #6b7280;"> <strong>Authors:</strong> Hadar Sharvit, Raanan Fattal, Matan Halfon </p> </div> <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This document excerpt presents introductory lecture notes for a course titled &quot;Intro to Deep Learning,&quot; developed by the Hebrew University of Jerusalem. The course material is based on lectures delivered by Raanan Fattal and recitations led by Matan Halfon. The notes cover fundamental concepts within deep learning, starting with information theory, specifically entropy and cross-entropy, and their relationship to likelihood maximization. A significant portion of the document details the architecture and training of neural networks, beginning with the Perceptron and progressing to Multi-Layer Perceptrons (MLPs). Key aspects covered include activation functions, various types, and their implementation. The document also delves into loss functions, differentiating between regression and classification losses. Finally, it explores the core training methodologies, including gradient descent optimization, mini-batching, and Stochastic Gradient Descent (SGD). The notes are presented as a non-formal course summary, with elaborations on important topics marked with a (‚Ä†) symbol. The document provides a foundational understanding of the building blocks of deep learning, offering a starting point for further study in this rapidly evolving field. The course aims to introduce students to the core concepts and techniques used in designing and training deep learning models. The inclusion of different loss functions highlights the importance of selecting the appropriate loss function based on the specific task at hand. The discussion of optimization algorithms like gradient descent and SGD underscores the critical role of efficient optimization in training deep learning models. The structure of the document clearly outlines the progression of topics, starting with theoretical foundations and moving towards practical training methods.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">Entropy and Cross-Entropy are related concepts in Information Theory.</li><li style="margin-bottom: 5px;">The Perceptron is a basic neural network model.</li><li style="margin-bottom: 5px;">Multi-Layer Perceptrons (MLP) are more complex neural network models.</li><li style="margin-bottom: 5px;">Activation Functions are used within neural networks.</li><li style="margin-bottom: 5px;">Loss Functions are used to evaluate the performance of neural networks.</li><li style="margin-bottom: 5px;">Gradient Descent is an optimization algorithm used in training neural networks.</li><li style="margin-bottom: 5px;">Unpaired Image-To-Image Translation</li><li style="margin-bottom: 5px;">Inception Score</li><li style="margin-bottom: 5px;">Frechet Inception Distance (FID)</li><li style="margin-bottom: 5px;">Bayes rule</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Deep Learning </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Information Theory </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Neural Network </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Gradient Descent </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Mini Batching </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> SGD </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Likelihood </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Regression Loss </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Classification Loss </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Cycle-GAN </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> AI </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Intro_to_Deep_Learning_Notes_by_Hadar_Sharvit_1642754732.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
üì• Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=8dee35d284a168ecc1d0070ef6994da6" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
üîç View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=8dee35d284a168ecc1d0070ef6994da6" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>