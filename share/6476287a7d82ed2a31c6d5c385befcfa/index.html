<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance ‚Äì their limited ability in spatial reasoning, a fundamental aspect of h..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models"><meta property="og:description" content="This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance ‚Äì their limited ability in spatial reasoning, a fundamental aspect of h..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/6476287a7d82ed2a31c6d5c385befcfa"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Technology"><meta property="article:tag" content="Large Language Models (LLMs), Spatial Reasoning, Visualization-of-Thought (VoT), Mind's Eye, Multimodal Large Language Models (MLLMs), Multi-Hop Reasoning, Visual Navigation, Visual Tiling, Chain-of-Thought (CoT) prompting, Intermediate Reasoning Steps"><meta property="article:published_time" content="2025-07-05T14:37:45.284989"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content="Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Cui Furu, Wei"><meta name="keywords" content="Large Language Models (LLMs), Spatial Reasoning, Visualization-of-Thought (VoT), Mind's Eye, Multimodal Large Language Models (MLLMs), Multi-Hop Reasoning, Visual Navigation, Visual Tiling, Chain-of-Thought (CoT) prompting, Intermediate Reasoning Steps"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/6476287a7d82ed2a31c6d5c385befcfa"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</h1> <div style="margin-bottom: 15px;"> <p style="color: #6b7280;"> <strong>Authors:</strong> Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Cui Furu, Wei </p> </div> <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance ‚Äì their limited ability in spatial reasoning, a fundamental aspect of human cognition. The core concept behind VoT is to visually represent the LLM&#39;s reasoning process, effectively mimicking the human ‚ÄòMind‚Äôs Eye‚Äô ‚Äì the ability to form mental images of unseen objects and actions. The authors propose VoT as a prompting technique that guides LLMs through multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling within 2D grid worlds.  The experimental results demonstrate a significant enhancement in the LLMs&#39; spatial reasoning abilities when utilizing VoT.  Crucially, VoT outperformed existing multimodal large language models (MLLMs) in these specific tasks, indicating a novel approach to boosting LLM performance. The research highlights the potential of leveraging visual representations to unlock more sophisticated reasoning skills in LLMs. The study‚Äôs findings suggest that the ‚ÄòMind‚Äôs Eye‚Äô concept, previously observed in human cognition, can be effectively translated into a prompting strategy for LLMs, opening avenues for future research in multimodal AI. The paper emphasizes the importance of visual grounding in LLMs to achieve more robust and accurate spatial reasoning, a key area for advancement in artificial intelligence. The success of VoT with LLMs provides a compelling case for exploring similar techniques to improve reasoning across various domains. The research also acknowledges the potential of adapting this approach for MLLMs, building upon the observed similarities between the LLM‚Äôs ‚ÄòMind‚Äôs Eye‚Äô and the human‚Äôs Mind‚Äôs Eye.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">LLMs exhibit limited spatial reasoning abilities.</li><li style="margin-bottom: 5px;">Visualization-of-Thought (VoT) prompting aims to elicit spatial reasoning by visualizing reasoning traces.</li><li style="margin-bottom: 5px;">The Mind‚Äôs Eye is a cognitive process enabling humans to create mental images, which we are attempting to replicate in LLMs.</li><li style="margin-bottom: 5px;">VoT prompting aims to generate reasoning traces and visualizations in an interleaved manner.</li><li style="margin-bottom: 5px;">CoT prompting generates a series of intermediate reasoning steps sequentially.</li><li style="margin-bottom: 5px;">The system visualizes the state after each reasoning step to aid spatial reasoning.</li><li style="margin-bottom: 5px;">The manipulation of mental image satisfies manipulation requirements (e.g., no overlapping, avoiding obstacles).</li><li style="margin-bottom: 5px;">The alignment of the mental image with the corresponding state.</li><li style="margin-bottom: 5px;">LLMs demonstrating promising capabilities in multi-hop visualization that adhere to spatial constraints.</li><li style="margin-bottom: 5px;">Grounded language model reasoning through simulation</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Large Language Models (LLMs) </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Spatial Reasoning </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Visualization-of-Thought (VoT) </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Mind&#39;s Eye </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Multimodal Large Language Models (MLLMs) </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Multi-Hop Reasoning </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Visual Navigation </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Visual Tiling </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Chain-of-Thought (CoT) prompting </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Intermediate Reasoning Steps </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> Technology </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Visualization_of_Thought_Eliciting_LLM_Spatial_Reasoning_1712488524.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
üì• Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
üîç View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>