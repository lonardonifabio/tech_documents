<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper addresses the critical security challenge posed by prompt injection attacks against AI agents powered by Large Language Models (LLMs). As LLMs are increasingly integrated into complex systems, particularly those granting agents tool access and handling sensitive information, t..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><title>Design Patterns for Securing LLM Agents against Prompt Injections</title><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Design Patterns for Securing LLM Agents against Prompt Injections"><meta property="og:description" content="This research paper addresses the critical security challenge posed by prompt injection attacks against AI agents powered by Large Language Models (LLMs). As LLMs are increasingly integrated into complex systems, particularly those granting agents tool access and handling sensitive information, t..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/share/4c9f149e213f9c63556fcd57489fadb4"><meta property="og:image" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:secure_url" content="https://www.fabiolonardoni.it/AIdatasciencelibrary_cover.JPG"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Design Patterns for Securing LLM Agents against Prompt Injections - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Technology"><meta property="article:tag" content="LLM Agents, Prompt Injection, Security, Design Patterns, Large Language Models, AI Agents, Tool Access, Natural Language Inputs, translator, actions"><meta property="article:published_time" content="2025-07-12T22:12:24"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><meta property="og:rich_attachment" content="true"><!-- Additional meta for mobile sharing --><meta name="author" content="Luca Beurer-Kellner, Beat Buesser, Ana-Maria Cret ¬∏u, Leonardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram `er, V¬¥aclav Volhejn"><meta name="keywords" content="LLM Agents, Prompt Injection, Security, Design Patterns, Large Language Models, AI Agents, Tool Access, Natural Language Inputs, translator, actions"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/share/4c9f149e213f9c63556fcd57489fadb4"><!-- Immediate redirect to main app --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=4c9f149e213f9c63556fcd57489fadb4";

      // Immediate redirect for users (crawlers will read meta tags first)
      if (typeof window !== 'undefined') {
        window.location.replace(mainAppUrl);
      }
    })();</script><!-- Fallback meta refresh for non-JS environments --><meta http-equiv="refresh" content="0; url={mainAppUrl}"><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css"></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div style="max-width: 800px; margin: 0 auto; padding: 20px; font-family: system-ui, sans-serif;"> <div style="background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"> <h1 style="color: #1f2937; margin-bottom: 20px;">Design Patterns for Securing LLM Agents against Prompt Injections</h1> <div style="margin-bottom: 15px;"> <p style="color: #6b7280;"> <strong>Authors:</strong> Luca Beurer-Kellner, Beat Buesser, Ana-Maria Cret ¬∏u, Leonardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram `er, V¬¥aclav Volhejn </p> </div> <div style="margin-bottom: 20px;"> <p style="color: #374151; line-height: 1.6;">This research paper addresses the critical security challenge posed by prompt injection attacks against AI agents powered by Large Language Models (LLMs). As LLMs are increasingly integrated into complex systems, particularly those granting agents tool access and handling sensitive information, the vulnerability to prompt injection attacks has become a significant concern. The authors propose a set of design patterns aimed at building AI agents with provable resistance to these attacks. The work systematically analyzes these patterns, carefully considering the trade-offs between security and the utility of the agents. Through a series of case studies, the research demonstrates the real-world applicability of these patterns. The paper highlights the importance of proactive security measures when designing LLM-based agents, emphasizing that simply relying on the inherent robustness of LLMs is insufficient. The research focuses on establishing a framework for building agents that can reliably resist manipulation through malicious prompts. The authors&#39; approach is intended to provide a practical and principled method for developers to mitigate the risks associated with prompt injection, contributing to the overall safety and trustworthiness of AI agent systems. The systematic analysis and case studies provide valuable insights for developers seeking to build secure and reliable LLM-powered agents, particularly in environments where data privacy and system integrity are paramount. The paper&#39;s findings are relevant to a wide range of applications, including those involving automated decision-making, intelligent assistants, and robotic systems.</p> </div> <div style="margin-bottom: 20px;"> <h3 style="color: #1f2937; margin-bottom: 10px;">Key Concepts:</h3> <ul style="color: #374151;"> <li style="margin-bottom: 5px;">As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their secu- rity has become a critical challenge.</li><li style="margin-bottom: 5px;">Among the most pressing threats are prompt injection attacks, which exploit the agent‚Äôs resilience on natural language inputs ‚Äî an especially dangerous threat when agents are granted tool access or handle sensitive information.</li><li style="margin-bottom: 5px;">We propose a set of principled design patterns for building AI agents with provable resistance to prompt injection.</li><li style="margin-bottom: 5px;">The Plan-Then-Execute Pattern allows an agent to formulate and execute a fixed plan of actions.</li><li style="margin-bottom: 5px;">Control flow integrity protects against prompt injections by preventing tool outputs from influencing the agent&#39;s actions.</li><li style="margin-bottom: 5px;">Prompt injections can still occur within the user prompt itself, despite the pattern&#39;s protective measures.</li><li style="margin-bottom: 5px;">The design exposes a large attack surface due to potential malicious instructions within files.</li><li style="margin-bottom: 5px;">User confirmation can improve security by requiring approval before LLM command execution.</li><li style="margin-bottom: 5px;">The action-selector pattern restricts LLM actions to predefined commands, acting as a translator.</li><li style="margin-bottom: 5px;">The plan-then-execute pattern guarantees execution of only required commands, mitigating prompt injection risks.</li> </ul> </div> <div style="margin-bottom: 20px;"> <span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> LLM Agents </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Prompt Injection </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Security </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Design Patterns </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Large Language Models </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> AI Agents </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Tool Access </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> Natural Language Inputs </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> translator </span><span style="background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 8px; margin-bottom: 8px; display: inline-block;"> actions </span> </div> <div style="margin-bottom: 20px;"> <span style="background: #dcfce7; color: #166534; padding: 4px 12px; border-radius: 20px; font-size: 14px; margin-right: 10px;"> Technology </span> <span style="background: #fed7aa; color: #9a3412; padding: 4px 12px; border-radius: 20px; font-size: 14px;"> Intermediate </span> </div> <div style="margin-bottom: 20px;"> <a href="https://raw.githubusercontent.com/lonardonifabio/tech_documents/main/documents/Design_Patterns_for_Securing_LLM_Agents_1752351144.pdf" target="_blank" rel="noopener noreferrer" style="background: #2563eb; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; margin-right: 10px; display: inline-block;">
üì• Download PDF
</a> <a href="https://lonardonifabio.github.io/tech_documents/?doc=4c9f149e213f9c63556fcd57489fadb4" style="background: #16a34a; color: white; padding: 10px 20px; border-radius: 8px; text-decoration: none; display: inline-block;">
üîç View in Library
</a> </div> <div style="background: #eff6ff; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6;"> <p style="color: #1e40af; font-size: 14px; margin: 0;"> <strong>Note:</strong> You will be automatically redirected to the interactive document library. 
            If the redirect doesn't work, <a href="https://lonardonifabio.github.io/tech_documents/?doc=4c9f149e213f9c63556fcd57489fadb4" style="color: #1e40af; text-decoration: underline;">click here</a>.
</p> </div> </div> </div> </body></html>