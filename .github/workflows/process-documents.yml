name: Process Documents with Ollama

on:
  push:
    branches: [ main, master ]
    paths:
      - 'documents/**'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'documents/**'
  workflow_dispatch: # Allow manual triggering

jobs:
  process-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Fetch all history for proper file tracking
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scripts/requirements.txt
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'package-lock.json'
    
    - name: Install Node.js dependencies
      run: npm ci
    
    - name: Install and start Ollama
      run: |
        # Install Ollama
        curl -fsSL https://ollama.ai/install.sh | sh
        
        # Generate a random port to avoid conflicts with other workflows
        OLLAMA_PORT=$((11434 + $RANDOM % 1000))
        echo "Using Ollama port: $OLLAMA_PORT"
        
        # Aggressive cleanup of any existing Ollama processes
        echo "Cleaning up any existing Ollama processes..."
        sudo pkill -f ollama || true
        sudo pkill -9 ollama || true
        sudo killall ollama || true
        sudo killall -9 ollama || true
        
        # Wait for cleanup
        sleep 5
        
        # Check and kill anything using the default port 11434
        if sudo lsof -ti:11434 2>/dev/null; then
          echo "Port 11434 is in use, force killing processes..."
          sudo kill -9 $(sudo lsof -ti:11434) || true
          sleep 3
        fi
        
        # Check our random port too
        if sudo lsof -ti:$OLLAMA_PORT 2>/dev/null; then
          echo "Port $OLLAMA_PORT is in use, force killing processes..."
          sudo kill -9 $(sudo lsof -ti:$OLLAMA_PORT) || true
          sleep 3
        fi
        
        # Start Ollama with custom port
        export OLLAMA_HOST=127.0.0.1:$OLLAMA_PORT
        export OLLAMA_ORIGINS="*"
        
        echo "Starting Ollama on port $OLLAMA_PORT..."
        ollama serve &
        OLLAMA_PID=$!
        echo "Started Ollama with PID: $OLLAMA_PID on port $OLLAMA_PORT"
        
        # Save the port for later steps
        echo "OLLAMA_PORT=$OLLAMA_PORT" >> $GITHUB_ENV
        echo "OLLAMA_HOST=127.0.0.1:$OLLAMA_PORT" >> $GITHUB_ENV
        
        # Wait for Ollama to be ready with extended timeout
        echo "Waiting for Ollama to start..."
        for i in {1..60}; do
          if curl -s http://127.0.0.1:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
            echo "Ollama is ready on port $OLLAMA_PORT!"
            break
          fi
          echo "Waiting... ($i/60)"
          sleep 3
        done
        
        # Verify Ollama is responding
        if ! curl -s http://127.0.0.1:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
          echo "ERROR: Ollama failed to start properly on port $OLLAMA_PORT"
          echo "Checking processes:"
          ps aux | grep ollama || true
          echo "Checking port usage:"
          sudo netstat -tlnp | grep $OLLAMA_PORT || true
          echo "Checking if service is running:"
          sudo systemctl status ollama || true
          exit 1
        fi
        
        # Pull Mistral model with retry logic
        echo "Pulling Mistral model..."
        for attempt in {1..3}; do
          echo "Attempt $attempt to pull mistral:7b..."
          if timeout 300 ollama pull mistral:7b; then
            echo "Mistral model pulled successfully!"
            break
          else
            echo "Attempt $attempt failed, retrying in 10 seconds..."
            sleep 10
          fi
          
          if [ $attempt -eq 3 ]; then
            echo "ERROR: Failed to pull model after 3 attempts"
            exit 1
          fi
        done
        
        # Verify model is available
        echo "Available models:"
        ollama list
        
        # Test model with a simple query
        echo "Testing Mistral model..."
        echo '{"model": "mistral:7b", "prompt": "Hello", "stream": false}' | curl -s -X POST http://127.0.0.1:$OLLAMA_PORT/api/generate -d @- || echo "Model test failed but continuing..."
    
    - name: Process documents with Ollama
      env:
        OLLAMA_MODEL: mistral:7b
      run: |
        echo "Processing documents with Ollama and Mistral..."
        python scripts/fixed_ollama_processor.py
        echo "Documents processed successfully"
    
    - name: Build website
      run: |
        echo "Building website..."
        npm run build
        echo "Website built successfully"
    
    - name: Commit updated documents.json if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Check if there are changes to commit
        if [[ -n $(git status --porcelain) ]]; then
          echo "Changes detected, committing..."
          git add data/documents.json data/processed_files.json
          if [ -d "dist/data" ]; then
            git add dist/data/documents.json
          fi
          git commit -m "Auto-update documents database with Ollama analysis [skip ci]" || echo "No changes to commit"
          git push || echo "Nothing to push"
        else
          echo "No changes to commit"
        fi
    
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.REP }}
        publish_dir: ./dist
        force_orphan: true
