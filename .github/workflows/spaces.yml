name: Deploy to GitHub Spaces

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Build project
      run: npm run build
      
    - name: Setup Ollama
      run: |
        # Install Ollama
        curl -fsSL https://ollama.ai/install.sh | sh
        
        # Start Ollama service in background
        nohup ollama serve > ollama.log 2>&1 &
        
        # Wait for service to start
        sleep 15
        
        # Pull lightweight model for GitHub Spaces
        ollama pull mistral:7b-instruct || ollama pull llama3.2:3b || echo "Model pull failed, will use default"
        
        # Verify installation
        ollama list || echo "Ollama list failed"
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./dist
        
    - name: Create Spaces configuration
      run: |
        mkdir -p .spaces
        cat > .spaces/config.yml << EOF
        # GitHub Spaces Configuration
        name: AI Document Library with Chat
        description: Document library with AI chat powered by Ollama/Mistral
        
        # Runtime configuration
        runtime:
          python_version: "3.9"
          node_version: "18"
          
        # Resource limits for free tier
        resources:
          memory: "8GB"
          cpu: "2"
          disk: "50GB"
          
        # Environment variables
        env:
          OLLAMA_BASE_URL: "http://localhost:11434"
          OLLAMA_DEFAULT_MODEL: "mistral:7b-instruct"
          NODE_ENV: "production"
          
        # Startup commands
        startup:
          - "curl -fsSL https://ollama.ai/install.sh | sh"
          - "nohup ollama serve &"
          - "sleep 10"
          - "ollama pull mistral:7b-instruct"
          - "npm install"
          - "npm run build"
          - "npm run preview"
          
        # Health check
        health_check:
          path: "/"
          interval: 30
          timeout: 10
          
        # Port configuration
        ports:
          - "4321:4321"  # Astro preview
          - "11434:11434"  # Ollama API
        EOF
        
    - name: Optimize for GitHub Spaces
      run: |
        # Create startup script
        cat > startup.sh << 'EOF'
        #!/bin/bash
        
        echo "ðŸš€ Starting AI Document Library..."
        
        # Install and start Ollama
        if ! command -v ollama &> /dev/null; then
            echo "ðŸ“¦ Installing Ollama..."
            curl -fsSL https://ollama.ai/install.sh | sh
        fi
        
        # Start Ollama service
        echo "ðŸ”§ Starting Ollama service..."
        nohup ollama serve > /tmp/ollama.log 2>&1 &
        
        # Wait for service
        echo "â³ Waiting for Ollama to start..."
        sleep 15
        
        # Pull model with fallback
        echo "ðŸ¤– Pulling AI model..."
        if ! ollama pull mistral:7b-instruct; then
            echo "âš ï¸  Mistral failed, trying smaller model..."
            ollama pull llama3.2:3b || echo "âŒ Model pull failed"
        fi
        
        # Start the web application
        echo "ðŸŒ Starting web application..."
        npm run preview --host 0.0.0.0 --port 4321
        EOF
        
        chmod +x startup.sh
        
    - name: Create Docker configuration (optional)
      run: |
        cat > Dockerfile << 'EOF'
        FROM node:18-alpine
        
        # Install system dependencies
        RUN apk add --no-cache curl bash
        
        # Install Ollama
        RUN curl -fsSL https://ollama.ai/install.sh | sh
        
        # Set working directory
        WORKDIR /app
        
        # Copy package files
        COPY package*.json ./
        
        # Install dependencies
        RUN npm ci --only=production
        
        # Copy application code
        COPY . .
        
        # Build application
        RUN npm run build
        
        # Expose ports
        EXPOSE 4321 11434
        
        # Create startup script
        COPY startup.sh /startup.sh
        RUN chmod +x /startup.sh
        
        # Start services
        CMD ["/startup.sh"]
        EOF
