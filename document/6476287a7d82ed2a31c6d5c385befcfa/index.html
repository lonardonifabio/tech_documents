<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance – their limited ability in spatial reasoning, a fundamental aspect of h..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models"><meta property="og:description" content="This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance – their limited ability in spatial reasoning, a fundamental aspect of h..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/6476287a7d82ed2a31c6d5c385befcfa"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/previews/6476287a7d82ed2a31c6d5c385befcfa.jpg?v=1751990647114"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/previews/6476287a7d82ed2a31c6d5c385befcfa.jpg?v=1751990647114"><meta property="og:image:type" content="image/svg+xml"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Technology"><meta property="article:tag" content="Large Language Models (LLMs), Spatial Reasoning, Visualization-of-Thought (VoT), Mind's Eye, Multimodal Large Language Models (MLLMs), Multi-Hop Reasoning, Visual Navigation, Visual Tiling, Chain-of-Thought (CoT) prompting, Intermediate Reasoning Steps"><meta property="article:published_time" content="2025-07-05T14:37:45.284989"><!-- Enhanced Twitter Card --><!-- <meta name="twitter:card" content="summary_large_image"> --><!-- <meta name="twitter:title" content={title}> --><!-- <meta name="twitter:description" content={description}> --><!-- <meta name="twitter:image" content={imageUrl}> --><!-- <meta name="twitter:image:alt" content={`${doc.title || doc.filename} - AI & Data Science Document Library`}> --><!-- <meta name="twitter:site" content="@fabiolonardoni"> --><!-- <meta name="twitter:creator" content="@fabiolonardoni"> --><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content="Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Cui Furu, Wei"><meta name="keywords" content="Large Language Models (LLMs), Spatial Reasoning, Visualization-of-Thought (VoT), Mind's Eye, Multimodal Large Language Models (MLLMs), Multi-Hop Reasoning, Visual Navigation, Visual Tiling, Chain-of-Thought (CoT) prompting, Intermediate Reasoning Steps"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/6476287a7d82ed2a31c6d5c385befcfa"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models","description":"This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance – their limited ability in spatial reasoning, a fundamental aspect of h...","author":{"@type":"Person","name":"Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Cui Furu, Wei"},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/previews/6476287a7d82ed2a31c6d5c385befcfa.jpg?v=1751990647114"}},"url":"https://lonardonifabio.github.io/tech_documents/document/6476287a7d82ed2a31c6d5c385befcfa","datePublished":"2025-07-05T14:37:45.284989","image":"https://lonardonifabio.github.io/tech_documents/previews/6476287a7d82ed2a31c6d5c385befcfa.jpg?v=1751990647114","keywords":"Large Language Models (LLMs), Spatial Reasoning, Visualization-of-Thought (VoT), Mind's Eye, Multimodal Large Language Models (MLLMs), Multi-Hop Reasoning, Visual Navigation, Visual Tiling, Chain-of-Thought (CoT) prompting, Intermediate Reasoning Steps"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</h1> <div class="mb-4"> <p class="text-gray-600"> <strong>Authors:</strong> Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Cui Furu, Wei </p> </div> <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This research paper investigates the application of Visualization-of-Thought (VoT) prompting to improve spatial reasoning capabilities within Large Language Models (LLMs). The study addresses a critical gap in LLM performance – their limited ability in spatial reasoning, a fundamental aspect of human cognition. The core concept behind VoT is to visually represent the LLM&#39;s reasoning process, effectively mimicking the human ‘Mind’s Eye’ – the ability to form mental images of unseen objects and actions. The authors propose VoT as a prompting technique that guides LLMs through multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling within 2D grid worlds.  The experimental results demonstrate a significant enhancement in the LLMs&#39; spatial reasoning abilities when utilizing VoT.  Crucially, VoT outperformed existing multimodal large language models (MLLMs) in these specific tasks, indicating a novel approach to boosting LLM performance. The research highlights the potential of leveraging visual representations to unlock more sophisticated reasoning skills in LLMs. The study’s findings suggest that the ‘Mind’s Eye’ concept, previously observed in human cognition, can be effectively translated into a prompting strategy for LLMs, opening avenues for future research in multimodal AI. The paper emphasizes the importance of visual grounding in LLMs to achieve more robust and accurate spatial reasoning, a key area for advancement in artificial intelligence. The success of VoT with LLMs provides a compelling case for exploring similar techniques to improve reasoning across various domains. The research also acknowledges the potential of adapting this approach for MLLMs, building upon the observed similarities between the LLM’s ‘Mind’s Eye’ and the human’s Mind’s Eye.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Large Language Models (LLMs) </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Spatial Reasoning </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Visualization-of-Thought (VoT) </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Mind&#39;s Eye </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Multimodal Large Language Models (MLLMs) </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Multi-Hop Reasoning </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Visual Navigation </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Visual Tiling </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Chain-of-Thought (CoT) prompting </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Intermediate Reasoning Steps </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> Technology </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
🔍 View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=6476287a7d82ed2a31c6d5c385befcfa" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 