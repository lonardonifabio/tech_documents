<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper, published on arXiv in June 2025 (arXiv:2505.24832v2 [cs.CL]), investigates the extent to which modern language models ‘memorize’ information from their training datasets. The authors propose a novel method to quantify this memorization, separating it into two distinct compone..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>How much do language models memorize?</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="How much do language models memorize?"><meta property="og:description" content="This research paper, published on arXiv in June 2025 (arXiv:2505.24832v2 [cs.CL]), investigates the extent to which modern language models ‘memorize’ information from their training datasets. The authors propose a novel method to quantify this memorization, separating it into two distinct compone..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/0e94a281e2c8875ee7e1ac00232933f2"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/previews/0e94a281e2c8875ee7e1ac00232933f2.jpg?v=1751980720911"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/previews/0e94a281e2c8875ee7e1ac00232933f2.jpg?v=1751980720911"><meta property="og:image:type" content="image/svg+xml"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="How much do language models memorize? - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Technology"><meta property="article:tag" content="language models, memorization, capacity, scaling laws, transformer language models, membership inference, data size, entropy, Kolmogorov Complexity, dataset"><meta property="article:published_time" content="2025-06-23T13:44:23.767407"><!-- Enhanced Twitter Card --><!-- <meta name="twitter:card" content="summary_large_image"> --><!-- <meta name="twitter:title" content={title}> --><!-- <meta name="twitter:description" content={description}> --><!-- <meta name="twitter:image" content={imageUrl}> --><!-- <meta name="twitter:image:alt" content={`${doc.title || doc.filename} - AI & Data Science Document Library`}> --><!-- <meta name="twitter:site" content="@fabiolonardoni"> --><!-- <meta name="twitter:creator" content="@fabiolonardoni"> --><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content=""><meta name="keywords" content="language models, memorization, capacity, scaling laws, transformer language models, membership inference, data size, entropy, Kolmogorov Complexity, dataset"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/0e94a281e2c8875ee7e1ac00232933f2"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"How much do language models memorize?","description":"This research paper, published on arXiv in June 2025 (arXiv:2505.24832v2 [cs.CL]), investigates the extent to which modern language models ‘memorize’ information from their training datasets. The authors propose a novel method to quantify this memorization, separating it into two distinct compone...","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/previews/0e94a281e2c8875ee7e1ac00232933f2.jpg?v=1751980720911"}},"url":"https://lonardonifabio.github.io/tech_documents/document/0e94a281e2c8875ee7e1ac00232933f2","datePublished":"2025-06-23T13:44:23.767407","image":"https://lonardonifabio.github.io/tech_documents/previews/0e94a281e2c8875ee7e1ac00232933f2.jpg?v=1751980720911","keywords":"language models, memorization, capacity, scaling laws, transformer language models, membership inference, data size, entropy, Kolmogorov Complexity, dataset"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=0e94a281e2c8875ee7e1ac00232933f2";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">How much do language models memorize?</h1>  <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This research paper, published on arXiv in June 2025 (arXiv:2505.24832v2 [cs.CL]), investigates the extent to which modern language models ‘memorize’ information from their training datasets. The authors propose a novel method to quantify this memorization, separating it into two distinct components: unintended memorization, representing the model&#39;s direct knowledge of the specific dataset, and generalization, which reflects the model&#39;s understanding of the underlying data generation process.  The core contribution is a framework for calculating total memorization, providing an estimate of model capacity.  The study utilizes a large-scale experiment training hundreds of transformer language models, ranging in size from 500K to 1.5B parameters, across datasets of increasing size.  The researchers observed a clear relationship between model size, data size, and memorization. Initially, models exhibited increasing memorization as their capacity filled. However, beyond a certain point, ‘grokking’ – a process where models begin to generalize – occurred, leading to a decrease in unintended memorization.  The findings suggest that models in the GPT family have an approximate capacity of 3.6 bits-per-parameter.  The research also explores scaling laws relating model capacity and data size to membership inference, a critical area for understanding and mitigating potential privacy risks associated with language models. The study&#39;s methodology and results offer valuable insights into the limitations and capabilities of current language models, particularly concerning their reliance on memorization rather than genuine understanding. The authors&#39; approach provides a more nuanced way to assess model capacity and highlights the importance of controlling for generalization to prevent unintended data leakage.  The research is significant for advancing the field of AI safety and responsible model development.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> language models </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> memorization </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> capacity </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> scaling laws </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> transformer language models </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> membership inference </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> data size </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> entropy </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Kolmogorov Complexity </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> dataset </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> Technology </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=0e94a281e2c8875ee7e1ac00232933f2" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
🔍 View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=0e94a281e2c8875ee7e1ac00232933f2" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 