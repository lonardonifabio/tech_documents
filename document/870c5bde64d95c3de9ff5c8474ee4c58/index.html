<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper investigates the extent to which language models memorize information from their training datasets. The authors introduce a novel method for quantifying memorization, distinguishing between 'unintended memorization' – the model's knowledge derived directly from the dataset – a..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>How much do language models memorize?</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="How much do language models memorize?"><meta property="og:description" content="This research paper investigates the extent to which language models memorize information from their training datasets. The authors introduce a novel method for quantifying memorization, distinguishing between 'unintended memorization' – the model's knowledge derived directly from the dataset – a..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/870c5bde64d95c3de9ff5c8474ee4c58"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/previews/870c5bde64d95c3de9ff5c8474ee4c58.jpg?v=1752494668640"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/previews/870c5bde64d95c3de9ff5c8474ee4c58.jpg?v=1752494668640"><meta property="og:image:type" content="image/svg+xml"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="How much do language models memorize? - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Technology"><meta property="article:tag" content="language models, memorization, capacity, scaling laws, transformer language models, data size, membership inference, GPT family, unintended memorization, entropy"><meta property="article:published_time" content="2025-06-23T22:00:43.412286"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content="John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar"><meta name="keywords" content="language models, memorization, capacity, scaling laws, transformer language models, data size, membership inference, GPT family, unintended memorization, entropy"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/870c5bde64d95c3de9ff5c8474ee4c58"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"How much do language models memorize?","description":"This research paper investigates the extent to which language models memorize information from their training datasets. The authors introduce a novel method for quantifying memorization, distinguishing between 'unintended memorization' – the model's knowledge derived directly from the dataset – a...","author":{"@type":"Person","name":"John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar"},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/previews/870c5bde64d95c3de9ff5c8474ee4c58.jpg?v=1752494668640"}},"url":"https://lonardonifabio.github.io/tech_documents/document/870c5bde64d95c3de9ff5c8474ee4c58","datePublished":"2025-06-23T22:00:43.412286","image":"https://lonardonifabio.github.io/tech_documents/previews/870c5bde64d95c3de9ff5c8474ee4c58.jpg?v=1752494668640","keywords":"language models, memorization, capacity, scaling laws, transformer language models, data size, membership inference, GPT family, unintended memorization, entropy"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=870c5bde64d95c3de9ff5c8474ee4c58";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">How much do language models memorize?</h1> <div class="mb-4"> <p class="text-gray-600"> <strong>Authors:</strong> John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar </p> </div> <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This research paper investigates the extent to which language models memorize information from their training datasets. The authors introduce a novel method for quantifying memorization, distinguishing between &#39;unintended memorization&#39; – the model&#39;s knowledge derived directly from the dataset – and &#39;generalization,&#39; which represents the model&#39;s understanding of the underlying data generation process.  By isolating and measuring unintended memorization, the researchers aim to determine the true capacity of modern language models, such as those within the GPT family.  Their experiments involved training hundreds of transformer language models, ranging in size from 500,000 to 1.5 billion parameters, across various dataset sizes (170K, 500K, 2.5M, 7M).  The study reveals a clear relationship between model size, data size, and memorization capacity. Initially, models exhibit increasing memorization as their capacity fills. However, beyond a certain point, &#39;grokking&#39; occurs, where unintended memorization plateaus, and the model begins to generalize, effectively reducing the amount of direct memorization.  The research culminates in an estimated capacity of approximately 3.6 bits-per-parameter for models in the GPT family.  The authors also explore scaling laws relating model capacity and data size to membership inference, providing valuable insights into the behavior of large language models. The paper&#39;s findings contribute to a better understanding of model capacity and the mechanisms underlying memorization in neural networks.  The research is significant for assessing the risks associated with model deployment and for guiding the development of more robust and generalizable language models.  The methodology and results presented offer a framework for future research in this critical area.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> language models </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> memorization </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> capacity </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> scaling laws </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> transformer language models </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> data size </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> membership inference </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> GPT family </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> unintended memorization </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> entropy </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> Technology </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=870c5bde64d95c3de9ff5c8474ee4c58" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
🔍 View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=870c5bde64d95c3de9ff5c8474ee4c58" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 