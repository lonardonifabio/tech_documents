<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document provides a concise cheatsheet focused on fundamental concepts within deep learning, specifically neural networks. It begins by introducing neural networks as layered models, highlighting common types like convolutional and recurrent networks. The core architecture is explained, deta..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>VIP Cheatsheet: Deep Learning</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="VIP Cheatsheet: Deep Learning"><meta property="og:description" content="This document provides a concise cheatsheet focused on fundamental concepts within deep learning, specifically neural networks. It begins by introducing neural networks as layered models, highlighting common types like convolutional and recurrent networks. The core architecture is explained, deta..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/a73b433aeffe70af420b0bf703fbd383"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/preview/a73b433aeffe70af420b0bf703fbd383.jpg?v=1751970970548"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/preview/a73b433aeffe70af420b0bf703fbd383.jpg?v=1751970970548"><meta property="og:image:type" content="image/svg+xml"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="VIP Cheatsheet: Deep Learning - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="neural networks, deep learning, activation functions, cross-entropy loss, learning rate, backpropagation, Adam"><meta property="article:published_time" content="2025-07-06T07:51:51.491388"><!-- Enhanced Twitter Card --><!-- <meta name="twitter:card" content="summary_large_image"> --><!-- <meta name="twitter:title" content={title}> --><!-- <meta name="twitter:description" content={description}> --><!-- <meta name="twitter:image" content={imageUrl}> --><!-- <meta name="twitter:image:alt" content={`${doc.title || doc.filename} - AI & Data Science Document Library`}> --><!-- <meta name="twitter:site" content="@fabiolonardoni"> --><!-- <meta name="twitter:creator" content="@fabiolonardoni"> --><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content="Afshine Amidi, Shervine Amidi"><meta name="keywords" content="neural networks, deep learning, activation functions, cross-entropy loss, learning rate, backpropagation, Adam"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/a73b433aeffe70af420b0bf703fbd383"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"VIP Cheatsheet: Deep Learning","description":"This document provides a concise cheatsheet focused on fundamental concepts within deep learning, specifically neural networks. It begins by introducing neural networks as layered models, highlighting common types like convolutional and recurrent networks. The core architecture is explained, deta...","author":{"@type":"Person","name":"Afshine Amidi, Shervine Amidi"},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/preview/a73b433aeffe70af420b0bf703fbd383.jpg?v=1751970970548"}},"url":"https://lonardonifabio.github.io/tech_documents/document/a73b433aeffe70af420b0bf703fbd383","datePublished":"2025-07-06T07:51:51.491388","image":"https://lonardonifabio.github.io/tech_documents/preview/a73b433aeffe70af420b0bf703fbd383.jpg?v=1751970970548","keywords":"neural networks, deep learning, activation functions, cross-entropy loss, learning rate, backpropagation, Adam"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=a73b433aeffe70af420b0bf703fbd383";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">VIP Cheatsheet: Deep Learning</h1> <div class="mb-4"> <p class="text-gray-600"> <strong>Authors:</strong> Afshine Amidi, Shervine Amidi </p> </div> <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This document provides a concise cheatsheet focused on fundamental concepts within deep learning, specifically neural networks. It begins by introducing neural networks as layered models, highlighting common types like convolutional and recurrent networks. The core architecture is explained, detailing the relationship between layers, hidden units, weights (w), biases (b), and the resulting output (z) represented as z[i] = w[i] * x[i] + b[i].  It then delves into activation functions, outlining popular choices such as Sigmoid, Tanh, ReLU, and Leaky ReLU, along with their respective mathematical formulations.  The document explains the use of cross-entropy loss (L(z, y)) in neural networks, defining it as L(z, y) = - [y * log(z) + (1 - y) * log(1 - z)]. Furthermore, it introduces the concept of the learning rate (Œ∑) and its importance in weight updates, mentioning the Adam optimization algorithm as a prevalent adaptive learning rate method. Finally, it briefly touches upon backpropagation, a key algorithm for updating network weights based on the difference between actual and desired outputs. The document serves as a quick reference for understanding essential elements of deep learning and neural network training.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> neural networks </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> deep learning </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> activation functions </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> cross-entropy loss </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> learning rate </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> backpropagation </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Adam </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> AI </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=a73b433aeffe70af420b0bf703fbd383" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
üîç View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=a73b433aeffe70af420b0bf703fbd383" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 