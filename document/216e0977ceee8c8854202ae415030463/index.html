<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This document provides a concise cheatsheet focused on unsupervised learning techniques, specifically utilizing the Expectation-Maximization (EM) algorithm and k-means clustering. It begins with a motivational introduction, highlighting the goal of uncovering hidden patterns within unlabeled data..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>VIP Cheatsheet: Unsupervised Learning</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="VIP Cheatsheet: Unsupervised Learning"><meta property="og:description" content="This document provides a concise cheatsheet focused on unsupervised learning techniques, specifically utilizing the Expectation-Maximization (EM) algorithm and k-means clustering. It begins with a motivational introduction, highlighting the goal of uncovering hidden patterns within unlabeled data..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/216e0977ceee8c8854202ae415030463"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/preview/216e0977ceee8c8854202ae415030463.jpg"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/preview/216e0977ceee8c8854202ae415030463.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="VIP Cheatsheet: Unsupervised Learning - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="Machine Learning"><meta property="article:tag" content="unsupervised learning, latent variables, Expectation-Maximization, k-means clustering, Jensen‚Äôs inequality, mixture of Gaussians, factor analysis, likelihood estimation"><meta property="article:published_time" content="2025-07-14T20:47:05.718903"><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content="Afshine Amidi, Shervine Amidi"><meta name="keywords" content="unsupervised learning, latent variables, Expectation-Maximization, k-means clustering, Jensen‚Äôs inequality, mixture of Gaussians, factor analysis, likelihood estimation"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/216e0977ceee8c8854202ae415030463"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"VIP Cheatsheet: Unsupervised Learning","description":"This document provides a concise cheatsheet focused on unsupervised learning techniques, specifically utilizing the Expectation-Maximization (EM) algorithm and k-means clustering. It begins with a motivational introduction, highlighting the goal of uncovering hidden patterns within unlabeled data...","author":{"@type":"Person","name":"Afshine Amidi, Shervine Amidi"},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/preview/216e0977ceee8c8854202ae415030463.jpg"}},"url":"https://lonardonifabio.github.io/tech_documents/document/216e0977ceee8c8854202ae415030463","datePublished":"2025-07-14T20:47:05.718903","image":"https://lonardonifabio.github.io/tech_documents/preview/216e0977ceee8c8854202ae415030463.jpg","keywords":"unsupervised learning, latent variables, Expectation-Maximization, k-means clustering, Jensen‚Äôs inequality, mixture of Gaussians, factor analysis, likelihood estimation"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=216e0977ceee8c8854202ae415030463";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">VIP Cheatsheet: Unsupervised Learning</h1> <div class="mb-4"> <p class="text-gray-600"> <strong>Authors:</strong> Afshine Amidi, Shervine Amidi </p> </div> <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This document provides a concise cheatsheet focused on unsupervised learning techniques, specifically utilizing the Expectation-Maximization (EM) algorithm and k-means clustering. It begins with a motivational introduction, highlighting the goal of uncovering hidden patterns within unlabeled datasets. A key concept introduced is Jensen&#39;s inequality, presented as a fundamental principle in probability and statistics. The document then delves into latent variables, defining them as hidden, unobserved variables often denoted as &#39;z&#39;, and outlines common settings where they appear, including mixtures of Gaussians and factor analysis. The core of the cheatsheet centers around the Expectation-Maximization (EM) algorithm, explaining its iterative process of constructing a lower bound on the likelihood function (E-step) and optimizing that bound (M-step).  The E-step involves calculating the posterior probability of each data point belonging to a particular cluster, while the M-step uses these probabilities as cluster-specific weights to re-estimate the cluster models.  The document also introduces k-means clustering as a practical application.  The overall aim is to provide a quick reference for understanding and applying these fundamental unsupervised learning methods. The document emphasizes the mathematical foundations, particularly Jensen&#39;s inequality, and the practical implementation of the EM algorithm, making it a valuable resource for students and practitioners in machine learning and data science.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> unsupervised learning </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> latent variables </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Expectation-Maximization </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> k-means clustering </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> Jensen‚Äôs inequality </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> mixture of Gaussians </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> factor analysis </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> likelihood estimation </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> Machine Learning </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=216e0977ceee8c8854202ae415030463" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
üîç View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=216e0977ceee8c8854202ae415030463" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 