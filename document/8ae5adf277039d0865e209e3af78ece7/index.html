<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncon..."><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/tech_documents/favicon.svg"><link rel="manifest" href="/tech_documents/manifest.json"><title>Two types of AI existential risk: decisive and accumulative</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-G7VY771Q53"></script><!-- PWA Meta Tags --><!-- <meta name="theme-color" content="#667eea"> --><!-- <meta name="apple-mobile-web-app-capable" content="yes"> --><!-- <meta name="apple-mobile-web-app-status-bar-style" content="default"> --><!-- <meta name="apple-mobile-web-app-title" content="AI Doc Library"> --><!-- Enhanced Open Graph for LinkedIn sharing --><meta property="og:title" content="Two types of AI existential risk: decisive and accumulative"><meta property="og:description" content="This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncon..."><meta property="og:type" content="article"><meta property="og:url" content="https://lonardonifabio.github.io/tech_documents/document/8ae5adf277039d0865e209e3af78ece7"><meta property="og:image" content="https://lonardonifabio.github.io/tech_documents/preview/8ae5adf277039d0865e209e3af78ece7.jpg?v=1751961066220"><meta property="og:image:secure_url" content="https://lonardonifabio.github.io/tech_documents/preview/8ae5adf277039d0865e209e3af78ece7.jpg?v=1751961066220"><meta property="og:image:type" content="image/svg+xml"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Two types of AI existential risk: decisive and accumulative - AI &#38; Data Science Document Library"><meta property="og:site_name" content="AI & Data Science Document Library"><meta property="og:locale" content="en_US"><meta property="article:author" content="Fabio Lonardoni"><meta property="article:section" content="AI"><meta property="article:tag" content="AI existential risk, x-risks, superintelligence, complex systems analysis, incremental disruptions, systemic resilience, AGI, ASI, instrumental convergence, paperclip production"><meta property="article:published_time" content="2025-06-27T22:32:59.411631"><!-- Enhanced Twitter Card --><!-- <meta name="twitter:card" content="summary_large_image"> --><!-- <meta name="twitter:title" content={title}> --><!-- <meta name="twitter:description" content={description}> --><!-- <meta name="twitter:image" content={imageUrl}> --><!-- <meta name="twitter:image:alt" content={`${doc.title || doc.filename} - AI & Data Science Document Library`}> --><!-- <meta name="twitter:site" content="@fabiolonardoni"> --><!-- <meta name="twitter:creator" content="@fabiolonardoni"> --><!-- LinkedIn specific meta tags --><meta property="linkedin:owner" content="Fabio Lonardoni"><!-- Additional meta for mobile sharing --><meta name="author" content="Atoosa Kasirzadeh"><meta name="keywords" content="AI existential risk, x-risks, superintelligence, complex systems analysis, incremental disruptions, systemic resilience, AGI, ASI, instrumental convergence, paperclip production"><meta name="robots" content="index, follow"><link rel="canonical" href="https://lonardonifabio.github.io/tech_documents/document/8ae5adf277039d0865e209e3af78ece7"><!-- Additional LinkedIn optimization --><meta property="og:rich_attachment" content="true"><!-- Structured Data for better SEO --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Two types of AI existential risk: decisive and accumulative","description":"This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncon...","author":{"@type":"Person","name":"Atoosa Kasirzadeh"},"publisher":{"@type":"Organization","name":"AI & Data Science Document Library","logo":{"@type":"ImageObject","url":"https://lonardonifabio.github.io/tech_documents/preview/8ae5adf277039d0865e209e3af78ece7.jpg?v=1751961066220"}},"url":"https://lonardonifabio.github.io/tech_documents/document/8ae5adf277039d0865e209e3af78ece7","datePublished":"2025-06-27T22:32:59.411631","image":"https://lonardonifabio.github.io/tech_documents/preview/8ae5adf277039d0865e209e3af78ece7.jpg?v=1751961066220","keywords":"AI existential risk, x-risks, superintelligence, complex systems analysis, incremental disruptions, systemic resilience, AGI, ASI, instrumental convergence, paperclip production"}</script><!-- Simple redirect script --><script>(function(){const mainAppUrl = "https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7";

      // Manual redirect only - no automatic redirects that interfere with crawlers
      if (typeof window !== 'undefined') {
        // Add click handler for manual redirect button
        document.addEventListener('DOMContentLoaded', () => {
          const redirectButton = document.getElementById('manual-redirect');
          if (redirectButton) {
            redirectButton.addEventListener('click', () => {
              window.location.href = mainAppUrl;
            });
          }
        });
      }
    })();</script><link rel="stylesheet" href="/tech_documents/assets/_id_.B2K7fHu1.css">
<style>html{font-family:system-ui,sans-serif}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:#f1f1f1}::-webkit-scrollbar-thumb{background:#c1c1c1;border-radius:4px}::-webkit-scrollbar-thumb:hover{background:#a8a8a8}
</style><script type="module">"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/tech_documents/sw.js").then(e=>{console.log("SW registered: ",e)}).catch(e=>{console.log("SW registration failed: ",e)})});window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-G7VY771Q53");
</script></head> <body class="bg-gray-50 min-h-screen"> <!-- Fallback content for crawlers and users with JS disabled --> <div class="max-w-4xl mx-auto p-8"> <div class="bg-white rounded-lg shadow-lg p-6"> <h1 class="text-3xl font-bold text-gray-900 mb-4">Two types of AI existential risk: decisive and accumulative</h1> <div class="mb-4"> <p class="text-gray-600"> <strong>Authors:</strong> Atoosa Kasirzadeh </p> </div> <div class="mb-6"> <p class="text-gray-700 leading-relaxed">This research paper critically examines the prevailing discourse surrounding existential risks (x-risks) posed by artificial intelligence (AI). The conventional understanding of AI x-risk predominantly centers on catastrophic, abrupt events stemming from advanced AI systems, often involving uncontrolled superintelligence and the potential for human extinction or irreversible societal collapse. However, the paper argues for a contrasting perspective ‚Äì an ‚Äòaccumulative AI x-risk‚Äô hypothesis. This hypothesis posits that existential threats from AI don&#39;t necessarily manifest through a single, dramatic takeover but rather through a gradual, incremental accumulation of vulnerabilities and disruptions. The author frames this as a ‚Äòboiling frog‚Äô scenario, where small, seemingly insignificant AI-induced risks accumulate over time, steadily eroding systemic and societal resilience. The paper highlights the danger of overlooking the potential for AI to destabilize economic and political structures through a series of interconnected, low-level threats. It emphasizes the importance of considering the long-term, gradual impacts of AI development, rather than solely focusing on immediate, dramatic scenarios. The research suggests that a failure to recognize this accumulative risk could lead to a situation where a triggering event, born from the accumulated vulnerabilities, results in an irreversible collapse of societal systems. The analysis draws upon complex systems theory to illustrate the dynamics of this potential threat, urging a more nuanced and comprehensive approach to assessing and mitigating AI x-risks.</p> </div> <div class="flex flex-wrap gap-2 mb-6"> <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> AI existential risk </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> x-risks </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> superintelligence </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> complex systems analysis </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> incremental disruptions </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> systemic resilience </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> AGI </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> ASI </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> instrumental convergence </span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm font-medium"> paperclip production </span> </div> <div class="flex items-center justify-between text-sm text-gray-500 mb-6"> <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full"> AI </span> <span class="bg-orange-100 text-orange-800 px-3 py-1 rounded-full"> Intermediate </span> </div> <div class="flex gap-4"> <a href="https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7" class="bg-green-600 text-white px-6 py-2 rounded-lg hover:bg-green-700 transition-colors">
üîç View in the library powered by AI
</a> </div> <div class="mt-6 p-4 bg-blue-50 rounded-lg"> <p class="text-blue-800 text-sm"> <strong>Note:</strong> This is a static page optimized for social media sharing.
<a href="https://lonardonifabio.github.io/tech_documents/?doc=8ae5adf277039d0865e209e3af78ece7" class="underline font-semibold">Click here to view this document in the interactive library</a>.
</p> </div> </div> </div> <!-- Service Worker Registration -->  </body> </html> 